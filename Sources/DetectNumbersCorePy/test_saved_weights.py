import numpy as np

from generated.best_weights import best_weights
from models.dense import DetectingDense, sigmoid, softmax
from utils.parse_csv import parse_csv


def test_saved_weights(input: np.ndarray, target: np.ndarray):
    layers = []
    weigths = np.array(best_weights, dtype=object)
    for i, denseWeights in enumerate(weigths):
        if (i < weigths.size - 1):
            layers.append(DetectingDense(denseWeights, sigmoid))
        else:
            layers.append(DetectingDense(denseWeights, softmax))

    result = input
    for layer in layers:
        result = layer.forward(result)

    digits = np.argmax(target, axis=0)
    predicted_digits = np.argmax(result, axis=0)
    prediction_accuracy = np.sum(
        digits == predicted_digits) / input.shape[1]

    return prediction_accuracy


#input, target = parse_csv("mnist_test")


input = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20833333333333334, 0.4861111111111111, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013888888888888888, 0.9375, 1.0, 0.7430555555555556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08333333333333333, 0.09722222222222222, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08333333333333333, 1.0, 1.0, 0.8263888888888888, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1736111111111111, 0.9652777777777778, 0.9791666666666666, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1875, 1.0, 1.0, 0.7222222222222222, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4305555555555556, 1.0, 1.0, 0.4930555555555556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3958333333333333, 1.0, 1.0, 0.5416666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.3402777777777778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5972222222222222, 1.0, 1.0, 0.3402777777777778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8819444444444444, 1.0, 0.9930555555555556, 0.05555555555555555, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8263888888888888, 1.0, 1.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1875, 1.0, 1.0, 0.875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06944444444444445, 0.9930555555555556, 1.0, 0.8958333333333334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5138888888888888, 1.0, 1.0, 0.5138888888888888, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3402777777777778, 1.0, 1.0, 0.6388888888888888, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5833333333333334, 1.0, 1.0, 0.2569444444444444, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6180555555555556, 1.0, 1.0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5833333333333334, 1.0, 1.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7847222222222222, 1.0, 1.0, 0.11805555555555555, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7152777777777778, 1.0, 1.0, 0.2152777777777778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8541666666666666, 1.0, 1.0, 0.027777777777777776, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8333333333333334, 1.0, 1.0, 0.06944444444444445, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013888888888888888, 0.9652777777777778, 1.0, 0.9583333333333334, 0.18055555555555555, 0.1388888888888889, 0.0625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7777777777777778, 1.0, 1.0, 0.11805555555555555, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11805555555555555, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9722222222222222, 0.8611111111111112, 0.7361111111111112, 0.5902777777777778, 0.4722222222222222, 0.8958333333333334, 1.0, 1.0, 0.20833333333333334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4236111111111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09722222222222222, 0.5833333333333334, 0.5833333333333334, 0.6388888888888888, 0.6805555555555556, 0.75, 0.8263888888888888, 0.9166666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.041666666666666664, 0.16666666666666666, 0.3333333333333333, 0.4166666666666667, 0.6875, 1.0, 1.0, 0.4375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375, 1.0, 1.0, 0.5277777777777778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2777777777777778, 1.0, 1.0, 0.5833333333333334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4097222222222222, 1.0, 1.0, 0.5416666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5833333333333334, 1.0, 1.0, 0.3680555555555556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5833333333333334, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6319444444444444, 1.0, 1.0, 0.3194444444444444, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8680555555555556, 1.0, 1.0, 0.10416666666666667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06944444444444445, 1.0, 1.0, 0.8611111111111112, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6319444444444444, 0.8958333333333334, 0.4097222222222222, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
input = np.reshape(input, (784, 1))

target = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])
target = np.reshape(target, (10, 1))

result = test_saved_weights(input, target)
print("Prediction accuracy %.1f" % (result * 100))
